{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Wkz12-1wMB58"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "with open('iris.csv', 'r') as f: \n",
        "  temp = np.genfromtxt(f, dtype='f4', delimiter=',')  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u511-PFYMFBg",
        "outputId": "cb09d553-72ea-4d0e-9ce1-aec001015d6c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_iter(batch_size, features, labels):\n",
        "    num_examples = len(features)\n",
        "    indices = list(range(num_examples))\n",
        "    # The examples are read at random, in no particular order\n",
        "    random.shuffle(indices)\n",
        "    for i in range(0, num_examples, batch_size):\n",
        "        j = tf.constant(indices[i:min(i + batch_size, num_examples)])\n",
        "        yield tf.gather(features, j), tf.gather(labels, j)"
      ],
      "metadata": {
        "id": "YG6NZeFuMHm6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimization algorithm Stochastic Gradient Descent\n",
        "def sgd(param, grad, lr, batch_size):\n",
        "  param.assign_sub(lr * grad)"
      ],
      "metadata": {
        "id": "UWgt2bO_MMyW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss Function\n",
        "def cross_entropy(z, t):\n",
        "  # return tf.keras.losses.CategoricalCrossentropy()(t,z)            \n",
        "  return  -(1/len(z))*tf.reduce_sum(tf.math.log(z)*t)"
      ],
      "metadata": {
        "id": "AGxi8YkMMbke"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(x,w):\n",
        "  # layer with softmax function  \n",
        "  u = np.hstack((np.ones((x.shape[0],1)), x))@w\n",
        "  u_exp = tf.math.exp(u)\n",
        "  z = u_exp/tf.reduce_sum(u_exp,axis=1,keepdims=True)\n",
        "  # if softmax activation is to be used, \n",
        "  # z = tf.nn.softmax(u)\n",
        "  return z"
      ],
      "metadata": {
        "id": "EVgBxa4SMdiB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GC7GOZCQbm7W"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Prep\n",
        "\n",
        "X = temp[:,0:-1]\n",
        "# One-hot output layer encodding\n",
        "labels = np.array(1*[temp[:,-1]==1, temp[:,-1]==2, temp[:,-1]==3]).T.astype('f4')\n",
        "\n",
        "# K: Nsamples, d: featureDimension, N: Nclasses\n",
        "K,d = X.shape\n",
        "N = labels.shape[1] \n",
        "\n",
        "# partition into 80/20% training/testing datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, train_size=0.8, shuffle=True, random_state=3, stratify=labels)"
      ],
      "metadata": {
        "id": "8ch-MgfHMnP6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9F_lmdYLdh4W"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "\n",
        "batch_size = 10\n",
        "lr = 0.03 # learning rate\n",
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "REz9Zvq0Mpn8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning\n",
        "w = tf.Variable(tf.random.normal(shape=(d+1,N)), trainable=True)\n",
        "for epoch in range(num_epochs):\n",
        "  for x, y in data_iter(batch_size, X_train, y_train):\n",
        "    # Feed-forward model\n",
        "    with tf.GradientTape() as g:\n",
        "      l = cross_entropy(model(x, w),y)\n",
        "    # Compute gradient on l with respect to w\n",
        "    dw = g.gradient(l, w)\n",
        "    # Update parameters using their gradient\n",
        "    sgd(w, dw, lr, batch_size)\n",
        "  # After one epoch, Evaluate resubstitution loss\n",
        "  train_loss = cross_entropy(model(tf.constant(X_train), w),y_train)\n",
        "  print(f'epoch {epoch + 1}, loss {float(tf.reduce_mean(train_loss)):f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMZgYhwdMfHj",
        "outputId": "5c47e5a0-9f73-4eb8-85d6-a9557502d1aa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, loss 1.305316\n",
            "epoch 2, loss 1.043620\n",
            "epoch 3, loss 0.812080\n",
            "epoch 4, loss 0.718381\n",
            "epoch 5, loss 0.612626\n",
            "epoch 6, loss 0.569255\n",
            "epoch 7, loss 0.532431\n",
            "epoch 8, loss 0.511231\n",
            "epoch 9, loss 0.504288\n",
            "epoch 10, loss 0.476407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "Z_test = model(X_test,w)\n",
        "idx = np.argmax(Z_test,axis=1)\n",
        "y_pred = np.zeros((idx.size, N))\n",
        "y_pred[np.arange(idx.size),idx] = 1\n",
        "Cmat = y_test.T@y_pred\n",
        "\n",
        "print('Confusion Matrix= \\n', Cmat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s10TsUqIMubA",
        "outputId": "f118d469-31bf-440d-f170-39d26852dadb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix= \n",
            " [[10.  0.  0.]\n",
            " [ 0.  9.  1.]\n",
            " [ 0.  2.  8.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ePAhuR2dBuU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}