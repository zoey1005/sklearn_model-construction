{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"36wZV4G372WT","executionInfo":{"status":"ok","timestamp":1677474720104,"user_tz":360,"elapsed":8730,"user":{"displayName":"YU HEN HU","userId":"04083449072545068296"}}},"source":["import tensorflow as tf\n","import numpy as np\n","import random\n","from google.colab import drive\n","from sklearn.model_selection import train_test_split"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UBPzbcsME_ei","outputId":"a920fef6-50f2-4795-8680-d79fb8981f23","executionInfo":{"status":"ok","timestamp":1677474740052,"user_tz":360,"elapsed":19965,"user":{"displayName":"YU HEN HU","userId":"04083449072545068296"}}},"source":["drive.mount('/content/drive')\n","with open('/content/drive/My Drive/Colab Notebooks/data/iris.csv', 'r') as f: \n","  temp = np.genfromtxt(f, dtype='f4', delimiter=',')  \n","  "],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"41FxqWjuDVcm","executionInfo":{"status":"ok","timestamp":1677474740052,"user_tz":360,"elapsed":7,"user":{"displayName":"YU HEN HU","userId":"04083449072545068296"}}},"source":["def data_iter(batch_size, features, labels):\n","    num_examples = len(features)\n","    indices = list(range(num_examples))\n","    # The examples are read at random, in no particular order\n","    random.shuffle(indices)\n","    for i in range(0, num_examples, batch_size):\n","        j = tf.constant(indices[i:min(i + batch_size, num_examples)])\n","        yield tf.gather(features, j), tf.gather(labels, j)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"XSCNT519s7xF","executionInfo":{"status":"ok","timestamp":1677474740053,"user_tz":360,"elapsed":7,"user":{"displayName":"YU HEN HU","userId":"04083449072545068296"}}},"source":["# Optimization algorithm Stochastic Gradient Descent\n","def sgd(param, grad, lr, batch_size):\n","  param.assign_sub(lr * grad)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"oplV28fQ0ZTy","executionInfo":{"status":"ok","timestamp":1677474740053,"user_tz":360,"elapsed":6,"user":{"displayName":"YU HEN HU","userId":"04083449072545068296"}}},"source":["# Loss Function\n","def cross_entropy(z, t):\n","  # return tf.keras.losses.CategoricalCrossentropy()(t,z)            \n","  return  -(1/len(z))*tf.reduce_sum(tf.math.log(z)*t)\n","    "],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"WVKNCTXuBB9A","executionInfo":{"status":"ok","timestamp":1677474740053,"user_tz":360,"elapsed":6,"user":{"displayName":"YU HEN HU","userId":"04083449072545068296"}}},"source":["def model(x,w):\n","  # layer with softmax function  \n","  u = np.hstack((np.ones((x.shape[0],1)), x))@w\n","  u_exp = tf.math.exp(u)\n","  z = u_exp/tf.reduce_sum(u_exp,axis=1,keepdims=True)\n","  # if softmax activation is to be used, \n","  # z = tf.nn.softmax(u)\n","  return z"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"ViCDki3gFPVg","executionInfo":{"status":"ok","timestamp":1677474740054,"user_tz":360,"elapsed":7,"user":{"displayName":"YU HEN HU","userId":"04083449072545068296"}}},"source":["# Data Prep\n","\n","X = temp[:,0:-1]\n","# One-hot output layer encodding\n","labels = np.array(1*[temp[:,-1]==1, temp[:,-1]==2, temp[:,-1]==3]).T.astype('f4')\n","\n","# K: Nsamples, d: featureDimension, N: Nclasses\n","K,d = X.shape\n","N = labels.shape[1] \n","\n","# partition into 80/20% training/testing datasets\n","X_train, X_test, y_train, y_test = train_test_split(X, labels, train_size=0.8, shuffle=True, random_state=3, stratify=labels)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"g0jpjWjTGP51","executionInfo":{"status":"ok","timestamp":1677474740054,"user_tz":360,"elapsed":6,"user":{"displayName":"YU HEN HU","userId":"04083449072545068296"}}},"source":["# Hyperparameters\n","\n","batch_size = 10\n","lr = 0.03 # learning rate\n","num_epochs = 10"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VYKa1O-iFDrK","outputId":"46869da1-3e49-4b12-fd96-9f1dfa714eca","executionInfo":{"status":"ok","timestamp":1677474740523,"user_tz":360,"elapsed":475,"user":{"displayName":"YU HEN HU","userId":"04083449072545068296"}}},"source":["# Learning\n","\n","w = tf.Variable(tf.random.normal(shape=(d+1,N)), trainable=True)\n","for epoch in range(num_epochs):\n","  for x, y in data_iter(batch_size, X_train, y_train):\n","    # Feed-forward model\n","    with tf.GradientTape() as g:\n","      l = cross_entropy(model(x, w),y)\n","    # Compute gradient on l with respect to w\n","    dw = g.gradient(l, w)\n","    # Update parameters using their gradient\n","    sgd(w, dw, lr, batch_size)\n","  # After one epoch, Evaluate resubstitution loss\n","  train_loss = cross_entropy(model(tf.constant(X_train), w),y_train)\n","  print(f'epoch {epoch + 1}, loss {float(tf.reduce_mean(train_loss)):f}')"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 1, loss 0.483166\n","epoch 2, loss 0.497220\n","epoch 3, loss 0.457452\n","epoch 4, loss 0.479404\n","epoch 5, loss 0.427217\n","epoch 6, loss 0.480973\n","epoch 7, loss 0.479022\n","epoch 8, loss 0.403329\n","epoch 9, loss 0.420230\n","epoch 10, loss 0.420937\n"]}]},{"cell_type":"code","metadata":{"id":"THNnf8ERXqE7"},"source":["# Evaluation\n","Z_test = model(X_test,w)\n","idx = np.argmax(Z_test,axis=1)\n","y_pred = np.zeros((idx.size, N))\n","y_pred[np.arange(idx.size),idx] = 1\n","Cmat = y_test.T@y_pred\n","\n","print('Confusion Matrix= \\n', Cmat)"],"execution_count":null,"outputs":[]}]}